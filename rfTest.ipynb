{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged.csv')\n",
    "\n",
    "X = df.drop(['label'], axis=1)  # Drop 'label' column\n",
    "y = df['label'] # Keep only labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data with specified ratios of malicious/benign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged.csv')\n",
    "\n",
    "# Split the dataset into malicious and benign subsets\n",
    "malicious_df = df[df['label'] == 1]\n",
    "benign_df = df[df['label'] == 0]\n",
    "\n",
    "# Desired test size and malicious ratio in test set\n",
    "test_size = 0.2 \n",
    "malicious_ratio_in_test = 0.1\n",
    "\n",
    "# Calculate the number of malicious samples in the test set\n",
    "total_test_samples = int(len(df) * test_size)\n",
    "malicious_test_samples = int(total_test_samples * malicious_ratio_in_test)\n",
    "benign_test_samples = total_test_samples - malicious_test_samples\n",
    "\n",
    "# Split the malicious data\n",
    "malicious_train, malicious_test = train_test_split(malicious_df, test_size=malicious_test_samples, random_state=42)\n",
    "\n",
    "# Split the benign data\n",
    "benign_train, benign_test = train_test_split(benign_df, test_size=benign_test_samples, random_state=42)\n",
    "\n",
    "# Combine the training and testing sets\n",
    "train_df = pd.concat([malicious_train, benign_train])\n",
    "test_df = pd.concat([malicious_test, benign_test])\n",
    "\n",
    "# Shuffle the training and testing sets to ensure random distribution\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separate features and labels for training and testing sets\n",
    "X_train = train_df.drop(['label'], axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop(['label'], axis=1)\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Print the sizes of the training and test data\n",
    "print(f'Training set size: {len(X_train)} samples')\n",
    "print(f'Test set size: {len(X_test)} samples')\n",
    "\n",
    "# Output the shapes to verify the splits\n",
    "print(f'Training set shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape}, {y_test.shape}')\n",
    "print(f'Class distribution in test set:\\n{y_test.value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train RF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importances = pd.Series(rf_model.feature_importances_, index=X.columns)\n",
    "print('Feature Importances:')\n",
    "print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
