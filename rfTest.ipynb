{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('merged.csv')\n",
    "\n",
    "# X = df.drop(['label'], axis=1)  # Drop 'label' column\n",
    "# y = df['label'] # Keep only labels\n",
    "\n",
    "# # Split data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data with specified ratios of malicious/benign data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features: 'name', 'packages', 'version', 'average_entropy', 'python_file_count', 'package_size', 'contains_ip', 'contains_domain', 'contains_bytestrings', 'contains_base64', 'contains_import_subprocess', 'contains_import_os', 'contains_import_network_modules'\n",
    "\n",
    "# Specify the columns to exclude\n",
    "columns_to_exclude =  ['name', 'packages', 'version', 'contains_domain', 'contains_base64']\n",
    "\n",
    "# Read the CSV file excluding specified columns\n",
    "df = pd.read_csv('merged.csv', usecols=lambda x: x not in columns_to_exclude)\n",
    "\n",
    "# Split the dataset into malicious and benign subsets\n",
    "malicious_df = df[df['label'] == 1]\n",
    "benign_df = df[df['label'] == 0]\n",
    "\n",
    "# Desired test size and malicious ratio in the test set\n",
    "test_size = 0.2 \n",
    "malicious_ratio_in_test = 0.1\n",
    "\n",
    "# Calculate the number of malicious samples in the test set\n",
    "total_test_samples = int(len(df) * test_size)\n",
    "malicious_test_samples = int(total_test_samples * malicious_ratio_in_test)\n",
    "benign_test_samples = total_test_samples - malicious_test_samples\n",
    "\n",
    "# Split the malicious data\n",
    "malicious_train, malicious_test = train_test_split(malicious_df, test_size=malicious_test_samples, random_state=42)\n",
    "\n",
    "# Split the benign data\n",
    "benign_train, benign_test = train_test_split(benign_df, test_size=benign_test_samples, random_state=42)\n",
    "\n",
    "# Combine the training and testing sets\n",
    "train_df = pd.concat([malicious_train, benign_train])\n",
    "test_df = pd.concat([malicious_test, benign_test])\n",
    "\n",
    "# Shuffle the training and testing sets to ensure random distribution\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Separate features and labels for training and testing sets\n",
    "X_train = train_df.drop(['label'], axis=1)\n",
    "y_train = train_df['label']\n",
    "X_test = test_df.drop(['label'], axis=1)\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Print the sizes of the training and test data\n",
    "print(f'Training set size: {len(X_train)} samples')\n",
    "print(f'Test set size: {len(X_test)} samples')\n",
    "\n",
    "# Output the shapes to verify the splits\n",
    "print(f'Training set shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test set shape: {X_test.shape}, {y_test.shape}')\n",
    "print(f'Class distribution in test set:\\n{y_test.value_counts()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train RF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "if columns_to_exclude:\n",
    "    remaining_columns = [col for col in X_train.columns if col not in columns_to_exclude]\n",
    "    if len(remaining_columns) == len(rf_model.feature_importances_):\n",
    "        feature_importances_adjusted = pd.Series(rf_model.feature_importances_, index=remaining_columns)\n",
    "        print('Adjusted Feature Importances:')\n",
    "        print(feature_importances_adjusted)\n",
    "    else:\n",
    "        print('Number of remaining columns does not match the length of feature importances.')\n",
    "else:\n",
    "    feature_importances = pd.Series(rf_model.feature_importances_, index=X_train.columns)\n",
    "    print('Feature Importances:')\n",
    "    print(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n",
    "\n",
    "print(f\"Flagged {results_df[results_df['Predicted'] == 1].shape[0]} out of {results_df[results_df['Actual'] == 1].shape[0]} malicious samples\")\n",
    "print(f\"Flagged {(results_df[results_df['Predicted'] == 1].shape[0] / len(X_train) ) * 100} percent of samples\")\n",
    "print(f\"{malicious_ratio_in_test * 100}% of test was malicious\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for the test set\n",
    "y_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "\n",
    "# Calculate the AUC (Area Under the Curve)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
