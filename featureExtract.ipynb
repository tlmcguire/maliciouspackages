{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import logging\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "logstd = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s:%(lineno)d - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S %Z\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[logstd]\n",
    ")\n",
    "\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count zipped packages to determine useable amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_dir = \"/mnt/volume_nyc1_01/Backstabbers-Knife-Collection/samples\"\n",
    "dataset_2_dir = \"/mnt/volume_nyc1_01/pypi_malregistry\"  \n",
    "\n",
    "def count_package_files(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tar.gz\") or file.endswith(\".tar.bz2\") or file.endswith(\".tar.xz\"):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "count_1 = count_package_files(dataset_1_dir)\n",
    "count_2 = count_package_files(dataset_2_dir)\n",
    "\n",
    "print(f\"Number of packages in dataset 1: {count_1}\")\n",
    "print(f\"Number of packages in dataset 2: {count_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count JSON files to determin useable amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_dir = \"/mnt/volume_nyc1_01/Backstabbers-Knife-Collection/samples\"\n",
    "dataset_2_dir = \"/mnt/volume_nyc1_01/benignPyPI\"\n",
    "\n",
    "def is_valid_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json.load(file)\n",
    "        return True\n",
    "    except (ValueError, json.JSONDecodeError):\n",
    "        return False\n",
    "\n",
    "def count_valid_json_files(directory):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == \"setup.json\" and is_valid_json_file(os.path.join(root, file)):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "count_1 = count_valid_json_files(dataset_1_dir)\n",
    "count_2 = count_valid_json_files(dataset_2_dir)\n",
    "\n",
    "print(f\"Number of valid setup.json files in dataset 1: {count_1}\")\n",
    "print(f\"Number of valid setup.json files in dataset 2: {count_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Shannon Entropy and append to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_setup_json_files(directory):\n",
    "    setup_json_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'setup.json':\n",
    "                setup_json_files.append(os.path.join(root, file))\n",
    "    return setup_json_files\n",
    "    \n",
    "setup_json_files = find_setup_json_files(dataset_dir)\n",
    "if setup_json_files:\n",
    "    print(\"Found setup.json files:\")\n",
    "    for file in setup_json_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No setup.json files found in the specified directory.\")\n",
    "\n",
    "\n",
    "def find_python_files(directory):\n",
    "    python_files = []\n",
    "    for entry in os.scandir(directory):\n",
    "        if entry.is_file() and entry.name.endswith('.py') and not entry.name.startswith('.'):\n",
    "            python_files.append(entry.path)\n",
    "        elif entry.is_dir():\n",
    "            python_files.extend(find_python_files(entry.path))\n",
    "    return python_files\n",
    "\n",
    "def shannon_entropy(directory):\n",
    "    package_entropies = {}\n",
    "    setup_json_files = find_setup_json_files(directory)\n",
    "    \n",
    "    for setup_file_path in setup_json_files:\n",
    "        package_path = os.path.dirname(setup_file_path)\n",
    "        package_name = os.path.basename(package_path)\n",
    "        \n",
    "        package_entropy = 0\n",
    "        total_files = 0\n",
    "        \n",
    "        python_files = find_python_files(package_path)\n",
    "        for file_path in python_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    text = f.read()\n",
    "                    freqs = np.array(list(Counter(text).values()))\n",
    "                    probs = freqs / len(text)\n",
    "                    entropy_value = entropy(probs, base=2)\n",
    "                    package_entropy += entropy_value\n",
    "                    total_files += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        if total_files > 0:\n",
    "            average_entropy = package_entropy / total_files\n",
    "            package_entropies[package_name] = average_entropy\n",
    "\n",
    "            try:\n",
    "                with open(setup_file_path, 'r+', encoding='utf-8', errors='ignore') as setup_file:\n",
    "                    try:\n",
    "                        setup_data = json.load(setup_file)\n",
    "                        setup_data[\"average_entropy\"] = average_entropy\n",
    "                        setup_file.seek(0)\n",
    "                        json.dump(setup_data, setup_file, indent=4)\n",
    "                        setup_file.truncate()\n",
    "                        print(f\"Updated {setup_file_path} with average entropy: {average_entropy}\")\n",
    "                    except json.JSONDecodeError as json_err:\n",
    "                        print(f\"JSON decode error in {setup_file_path}: {json_err}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating {setup_file_path}: {e}\")\n",
    "\n",
    "    return package_entropies\n",
    "\n",
    "package_entropies = shannon_entropy(dataset_dir)\n",
    "for package, entropy in package_entropies.items():\n",
    "    print(f\"Shannon entropy of {package}: {entropy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct AST, store in XML, parse features, append features to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_python_files(directory):\n",
    "    python_files = []\n",
    "    for entry in os.scandir(directory):\n",
    "        if entry.is_file() and entry.name.endswith('.py') and not entry.name.startswith('.'):\n",
    "            python_files.append(entry.path)\n",
    "        elif entry.is_dir():\n",
    "            python_files.extend(find_python_files(entry.path))\n",
    "    return python_files\n",
    "\n",
    "def construct_ast(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            return tree\n",
    "        except SyntaxError as e:\n",
    "            print(f\"SyntaxError in {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def ast_to_xml(node):\n",
    "    def _convert(node, parent):\n",
    "        if isinstance(node, ast.AST):\n",
    "            node_name = node.__class__.__name__\n",
    "            element = ET.SubElement(parent, node_name)\n",
    "            for field, value in ast.iter_fields(node):\n",
    "                field_elem = ET.SubElement(element, field)\n",
    "                _convert(value, field_elem)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                item_elem = ET.SubElement(parent, 'item')\n",
    "                _convert(item, item_elem)\n",
    "        else:\n",
    "            parent.text = str(node)\n",
    "    root = ET.Element(node.__class__.__name__)\n",
    "    _convert(node, root)\n",
    "    return root\n",
    "\n",
    "dataset_dir = \"/mnt/volume_nyc1_01/benignPyPI\"\n",
    "\n",
    "python_files = find_python_files(dataset_dir)\n",
    "\n",
    "for file in python_files:\n",
    "    tree = construct_ast(file)\n",
    "    if tree is not None:\n",
    "        print(f\"Abstract syntax tree of {file}:\")\n",
    "        print(ast.dump(tree, indent=2))\n",
    "        print()\n",
    "        xml = ast_to_xml(tree)\n",
    "        xml_str = ET.tostring(xml, encoding='unicode', method='xml')\n",
    "        print(f\"XML representation of {file}:\")\n",
    "        print(xml_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to dataframe with option to save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_files(directory):\n",
    "    # Initialize an empty list to hold the JSON data\n",
    "    json_data_list = []\n",
    "\n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'setup.json':\n",
    "                # Construct the full file path\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read the JSON file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    json_data_list.append(json_data)\n",
    "\n",
    "    # Convert the list of JSON data to a DataFrame\n",
    "    df = pd.DataFrame(json_data_list)\n",
    "    return df\n",
    "\n",
    "# Specify the directory containing the packages\n",
    "directory = '/mnt/volume_nyc1_01/benignPyPI'\n",
    "\n",
    "# Call the function and get the DataFrame\n",
    "df = read_json_files(directory)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n",
    "\n",
    "#save to CSV\n",
    "df.to_csv('benignPyPI.csv', index=False)\n",
    "\n",
    "#load df from CSV\n",
    "df = pd.read_csv('benignPyPI.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
