{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import logging\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "\n",
    "logstd = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s:%(lineno)d - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S %Z\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[logstd]\n",
    ")\n",
    "\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_setup_json_files(directory):\n",
    "    setup_json_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'setup.json':\n",
    "                setup_json_files.append(os.path.join(root, file))\n",
    "    return setup_json_files\n",
    "\n",
    "def is_valid_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json.load(file)\n",
    "        return True\n",
    "    except (ValueError, json.JSONDecodeError):\n",
    "        return False\n",
    "\n",
    "def count_valid_json_files(directory):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == \"setup.json\" and is_valid_json_file(os.path.join(root, file)):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def count_package_files(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tar.gz\") or file.endswith(\".tar.bz2\") or file.endswith(\".tar.xz\"):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def is_valid_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json.load(file)\n",
    "        return True\n",
    "    except (ValueError, json.JSONDecodeError):\n",
    "        return False\n",
    "\n",
    "def count_valid_json_files(directory):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == \"setup.json\" and is_valid_json_file(os.path.join(root, file)):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def find_setup_json_files(directory):\n",
    "    setup_json_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'setup.json':\n",
    "                setup_json_files.append(os.path.join(root, file))\n",
    "    return setup_json_files\n",
    "\n",
    "def find_python_files(directory):\n",
    "    python_files = []\n",
    "    for entry in os.scandir(directory):\n",
    "        if entry.is_file() and entry.name.endswith('.py') and not entry.name.startswith('.'):\n",
    "            python_files.append(entry.path)\n",
    "        elif entry.is_dir():\n",
    "            python_files.extend(find_python_files(entry.path))\n",
    "    return python_files\n",
    "\n",
    "def read_json_files(directory):\n",
    "    json_data_list = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'setup.json':\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    print(f\"Read {file_path}: {json_data}\")  # Debug statement\n",
    "                    json_data_list.append(json_data)\n",
    "\n",
    "    df = pd.DataFrame(json_data_list)\n",
    "    return df\n",
    "\n",
    "\n",
    "dataset_dir = \"/mnt/volume_nyc1_01/Backstabbers-Knife-Collection/samples/pypi/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count zipped packages to determine useable amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_dir = \"/mnt/volume_nyc1_01/Backstabbers-Knife-Collection/samples\"\n",
    "dataset_2_dir = \"/mnt/volume_nyc1_01/pypi_malregistry\"  \n",
    "\n",
    "count_1 = count_package_files(dataset_1_dir)\n",
    "count_2 = count_package_files(dataset_2_dir)\n",
    "\n",
    "print(f\"Number of packages in dataset 1: {count_1}\")\n",
    "print(f\"Number of packages in dataset 2: {count_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count JSON files to determin useable amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_dir = \"/mnt/volume_nyc1_01/Backstabbers-Knife-Collection/samples\"\n",
    "dataset_2_dir = \"/mnt/volume_nyc1_01/pypi_malregistry\"  \n",
    "\n",
    "count_1 = count_valid_json_files(dataset_1_dir)\n",
    "count_2 = count_valid_json_files(dataset_2_dir)\n",
    "\n",
    "print(f\"Number of valid setup.json files in dataset 1: {count_1}\")\n",
    "print(f\"Number of valid setup.json files in dataset 2: {count_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Shannon Entropy and append to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(directory):\n",
    "    package_entropies = {}\n",
    "    setup_json_files = find_setup_json_files(directory)\n",
    "    \n",
    "    for setup_file_path in setup_json_files:\n",
    "        package_path = os.path.dirname(setup_file_path)\n",
    "        package_name = os.path.basename(package_path)\n",
    "        \n",
    "        package_entropy = 0\n",
    "        total_files = 0\n",
    "        \n",
    "        python_files = find_python_files(package_path)\n",
    "        for file_path in python_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    text = f.read()\n",
    "                    freqs = np.array(list(Counter(text).values()))\n",
    "                    probs = freqs / len(text)\n",
    "                    entropy_value = entropy(probs, base=2)\n",
    "                    package_entropy += entropy_value\n",
    "                    total_files += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        if total_files > 0:\n",
    "            average_entropy = package_entropy / total_files\n",
    "            package_entropies[package_name] = average_entropy\n",
    "\n",
    "            try:\n",
    "                with open(setup_file_path, 'r+', encoding='utf-8', errors='ignore') as setup_file:\n",
    "                    try:\n",
    "                        setup_data = json.load(setup_file)\n",
    "                        setup_data[\"average_entropy\"] = average_entropy\n",
    "                        setup_file.seek(0)\n",
    "                        json.dump(setup_data, setup_file, indent=4)\n",
    "                        setup_file.truncate()\n",
    "                        print(f\"Updated {setup_file_path} with average entropy: {average_entropy}\")\n",
    "                    except json.JSONDecodeError as json_err:\n",
    "                        print(f\"JSON decode error in {setup_file_path}: {json_err}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating {setup_file_path}: {e}\")\n",
    "\n",
    "    return package_entropies\n",
    "\n",
    "setup_json_files = find_setup_json_files(dataset_dir)\n",
    "\n",
    "if setup_json_files:\n",
    "    print(\"Found setup.json files:\")\n",
    "    for file in setup_json_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No setup.json files found in the specified directory.\")\n",
    "\n",
    "package_entropies = shannon_entropy(dataset_dir)\n",
    "for package, entropy in package_entropies.items():\n",
    "    print(f\"Shannon entropy of {package}: {entropy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct AST and store in XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ast(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            return tree\n",
    "        except SyntaxError as e:\n",
    "            print(f\"SyntaxError in {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def _convert(node, parent):\n",
    "    if isinstance(node, ast.AST):\n",
    "        node_name = node.__class__.__name__\n",
    "        element = ET.SubElement(parent, node_name)\n",
    "        for field, value in ast.iter_fields(node):\n",
    "            field_elem = ET.SubElement(element, field)\n",
    "            _convert(value, field_elem)\n",
    "    elif isinstance(node, list):\n",
    "        for item in node:\n",
    "            item_elem = ET.SubElement(parent, 'item')\n",
    "            _convert(item, item_elem)\n",
    "    else:\n",
    "        parent.text = str(node)\n",
    "\n",
    "def ast_to_xml(node):\n",
    "    root = ET.Element(node.__class__.__name__)\n",
    "    _convert(node, root)\n",
    "    return root\n",
    "\n",
    "def find_python_files(directory):\n",
    "    python_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                python_files.append(os.path.join(root, file))\n",
    "    return python_files\n",
    "\n",
    "def save_xml(xml, file_path):\n",
    "    xml_str = ET.tostring(xml, encoding='unicode', method='xml')\n",
    "    xml_file_path = os.path.splitext(file_path)[0] + '.xml'\n",
    "    with open(xml_file_path, 'w', encoding='utf-8', errors='replace') as f:\n",
    "        f.write(xml_str)\n",
    "\n",
    "python_files = find_python_files(dataset_dir)\n",
    "\n",
    "for file in python_files:\n",
    "    tree = construct_ast(file)\n",
    "    if tree is not None:\n",
    "        xml = ast_to_xml(tree)\n",
    "        save_xml(xml, file)\n",
    "        print(f\"XML representation saved for {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_python_files(directory):\n",
    "    python_file_count = 0\n",
    "    for entry in os.scandir(directory):\n",
    "        if entry.is_file() and entry.name.endswith('.py'):\n",
    "            python_file_count += 1\n",
    "        elif entry.is_dir():\n",
    "            python_file_count += count_python_files(entry.path)\n",
    "    return python_file_count\n",
    "\n",
    "def count_python_files_in_packages(directory):\n",
    "    package_counts = {}\n",
    "    \n",
    "    print(f\"Scanning dataset directory: {directory}\")\n",
    "    for entry in os.scandir(directory):\n",
    "        if entry.is_dir():\n",
    "            package_path = entry.path\n",
    "            print(f\"Scanning package directory: {package_path}\")\n",
    "            python_files_count = count_python_files(package_path)\n",
    "            package_name = entry.name\n",
    "            package_counts[package_name] = python_files_count\n",
    "            \n",
    "            setup_json_files = find_setup_json_files(package_path)\n",
    "            if setup_json_files:\n",
    "                for setup_json_path in setup_json_files:\n",
    "                    try:\n",
    "                        with open(setup_json_path, 'r') as file:\n",
    "                            data = json.load(file)\n",
    "                        \n",
    "                        data['python_file_count'] = python_files_count\n",
    "\n",
    "                        with open(setup_json_path, 'w') as file:\n",
    "                            json.dump(data, file, indent=4)\n",
    "\n",
    "                        print(f\"Updated {setup_json_path} with python_file_count: {python_files_count}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error updating {setup_json_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"No setup.json found in {package_path}\")\n",
    "        else:\n",
    "            print(f\"Skipping non-directory entry: {entry.name}\")\n",
    "    \n",
    "    return package_counts\n",
    "\n",
    "package_python_file_counts = count_python_files_in_packages(dataset_dir)\n",
    "\n",
    "for package, count in package_python_file_counts.items():\n",
    "    print(f\"Package '{package}' has {count} Python files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the size of each package in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_package_sizes(directory):\n",
    "    package_sizes = {}\n",
    "\n",
    "    # Find all setup.json files in the directory\n",
    "    setup_json_files = find_setup_json_files(directory)\n",
    "\n",
    "    for setup_json_path in setup_json_files:\n",
    "        package_dir = os.path.dirname(setup_json_path)\n",
    "        package_name = os.path.basename(package_dir)\n",
    "\n",
    "        # Filter out tarfiles (.tar.gz, .tar.bz2, .tar.xz)\n",
    "        filtered_files = []\n",
    "        for root, dirs, files in os.walk(package_dir):\n",
    "            filtered_files += [os.path.join(root, filename) for filename in files \n",
    "                               if not (filename.endswith('.tar.gz') or filename.endswith('.tar.bz2') or filename.endswith('.tar.xz'))]\n",
    "\n",
    "        # Calculate size of remaining files\n",
    "        package_size = sum(os.path.getsize(filename) for filename in filtered_files)\n",
    "        package_sizes[package_name] = package_size\n",
    "\n",
    "        try:\n",
    "            # Read the existing setup.json file\n",
    "            with open(setup_json_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            print(f\"Original setup.json data for {package_name}: {data}\")\n",
    "\n",
    "            # Append the package size to the setup.json data\n",
    "            data['package_size'] = package_size\n",
    "            print(f\"Updated setup.json data for {package_name}: {data}\")\n",
    "\n",
    "            # Write the updated data back to setup.json\n",
    "            with open(setup_json_path, 'w') as f:\n",
    "                json.dump(data, f, indent=4)\n",
    "            print(f\"Successfully updated {setup_json_path} with package size: {package_size}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating {setup_json_path}: {e}\")\n",
    "\n",
    "    return package_sizes\n",
    "\n",
    "# Calculate and print out the package sizes\n",
    "dataset_dir = \"/mnt/volume_nyc1_01/benignPyPI/\"\n",
    "package_sizes = calculate_package_sizes(dataset_dir)\n",
    "for package_name, size_in_bytes in package_sizes.items():\n",
    "    print(f\"Package '{package_name}' size: {size_in_bytes} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unwanted keys from setup.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove__key(file_path, key):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        if key in data:\n",
    "            del data[key]\n",
    "            \n",
    "            with open(file_path, 'w') as file:\n",
    "                json.dump(data, file, indent=4)\n",
    "                \n",
    "            print(f\"Removed {key} from {file_path}\")\n",
    "        else:\n",
    "            print(f\" {key} not found in {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Find all setup.json files in the directory\n",
    "setup_json_files = find_setup_json_files(dataset_dir)\n",
    "\n",
    "# Remove the 'package_size_bytes' key from each setup.json file\n",
    "for file_path in setup_json_files:\n",
    "    remove__key(file_path, 'containsIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepend the directory name to setup.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepend_directory_name_to_setup_json(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if 'setup.json' in files:\n",
    "            setup_json_path = os.path.join(root, 'setup.json')\n",
    "            \n",
    "            # Extract the relative path and split to find the correct directory name\n",
    "            relative_path = os.path.relpath(root, directory)\n",
    "            directory_name = relative_path.split(os.sep)[0]\n",
    "            \n",
    "            # Read the existing content of the setup.json file\n",
    "            with open(setup_json_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                data = json.loads(content)\n",
    "            \n",
    "            # Prepend the directory name\n",
    "            data = {\"directory_name\": directory_name, **data}\n",
    "            \n",
    "            # Write the updated content back to the setup.json file\n",
    "            with open(setup_json_path, 'w') as file:\n",
    "                json.dump(data, file, indent=4)\n",
    "            \n",
    "            print(f\"Prepended directory name '{directory_name}' to {setup_json_path}\")\n",
    "\n",
    "prepend_directory_name_to_setup_json(dataset_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse .xml files for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_xml(directory):\n",
    "    setup_json_files = find_setup_json_files(directory)\n",
    "\n",
    "    for package in os.listdir(directory):\n",
    "        package_path = os.path.join(directory, package)\n",
    "        if not os.path.isdir(package_path):\n",
    "            continue\n",
    "\n",
    "        xml_files = []\n",
    "\n",
    "        for root, dirs, files in os.walk(package_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".xml\"):\n",
    "                    xml_file_path = os.path.join(root, file)\n",
    "                    print(f\"Found XML file: {xml_file_path}\")\n",
    "\n",
    "                    # append XML file path to the list\n",
    "                    xml_files.append(xml_file_path)\n",
    "\n",
    "        if xml_files:\n",
    "            print(f\"XML files found in {package_path}: {xml_files}\")\n",
    "        else:\n",
    "            print(f\"No XML files found in {package_path}\")\n",
    "\n",
    "        # Features to extract\n",
    "        contains_ip = 0\n",
    "        contains_domain = 0\n",
    "        contains_bytestrings = 0\n",
    "        contains_base64 = 0\n",
    "        contains_eval = 0\n",
    "        contains_import_subprocess = 0\n",
    "        contains_import_os = 0\n",
    "        contains_import_network_modules = 0\n",
    "        contains_os_environ_access = 0\n",
    "\n",
    "        # Patterns\n",
    "        ip_address_pattern = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "        domain_pattern = re.compile(r'\\b(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}\\b')\n",
    "        bytestring_pattern = re.compile(r\"b'[^']*'\")\n",
    "        base64_pattern = re.compile(r'(?:(?:[A-Za-z0-9+/]{4})*(?:[A-Za-z0-9+/]{2}==|[A-Za-z0-9+/]{3}=)?)')\n",
    "\n",
    "        for xml_file in xml_files:\n",
    "            try:\n",
    "                tree = ET.parse(xml_file)\n",
    "                xml_root = tree.getroot()\n",
    "                print(f\"Successfully parsed {xml_file}\")\n",
    "\n",
    "                # Extract the features from the XML file\n",
    "                for child in xml_root.iter():\n",
    "                    # Check for IP address in the XML element text\n",
    "                    if child.text:\n",
    "                        # Check for IP address\n",
    "                        if not contains_ip:\n",
    "                            ips = ip_address_pattern.findall(child.text)\n",
    "                            if ips:\n",
    "                                contains_ip = 1\n",
    "\n",
    "                        # Check for domain\n",
    "                        if not contains_domain:\n",
    "                            domains = domain_pattern.findall(child.text)\n",
    "                            if domains:\n",
    "                                contains_domain = 1\n",
    "\n",
    "                        # Check for bytestring\n",
    "                        if not contains_bytestrings:\n",
    "                            bytestrings = bytestring_pattern.findall(child.text)\n",
    "                            if bytestrings:\n",
    "                                contains_bytestrings = 1\n",
    "\n",
    "                        # Check for base64\n",
    "                        if not contains_base64:\n",
    "                            base64s = base64_pattern.findall(child.text)\n",
    "                            if base64s:\n",
    "                                contains_base64 = 1\n",
    "\n",
    "                        # If all features are found, no need to continue\n",
    "                        if contains_ip and contains_domain and contains_bytestrings and contains_base64:\n",
    "                            break\n",
    "\n",
    "                    # Check for eval\n",
    "                    if child.tag == 'Call' and 'func' in child.attrib and child.attrib['func'] == 'eval':\n",
    "                        contains_eval = 1\n",
    "                    \n",
    "                    # Check for Python imports\n",
    "                    if child.tag == 'Import' or child.tag == 'ImportFrom':\n",
    "                        for grandchild in child:\n",
    "                            if 'name' in grandchild.attrib:\n",
    "                                if grandchild.attrib['name'] == 'subprocess':\n",
    "                                    contains_import_subprocess = 1\n",
    "                                elif grandchild.attrib['name'] == 'os':\n",
    "                                    contains_import_os = 1\n",
    "                                elif grandchild.attrib['name'] in ['socket', 'requests', 'http', 'urllib']:\n",
    "                                    contains_import_network_modules = 1\n",
    "\n",
    "                    # Check for os.environ access\n",
    "                    if child.tag == 'Attribute' and 'attr' in child.attrib and child.attrib['attr'] == 'environ':\n",
    "                        if 'value' in child.attrib and 'os' in child.attrib['value']:\n",
    "                            contains_os_environ_access = 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {xml_file}: {e}\")\n",
    "\n",
    "        # Find the corresponding setup.json file for this package\n",
    "        setup_json_file = None\n",
    "        for file_path in setup_json_files:\n",
    "            if file_path.startswith(package_path):\n",
    "                setup_json_file = file_path\n",
    "                break\n",
    "\n",
    "        if setup_json_file:\n",
    "            try:\n",
    "                with open(setup_json_file, 'r') as f:\n",
    "                    setup_data = json.load(f)\n",
    "\n",
    "                # Append extracted features to setup_data\n",
    "                setup_data['contains_ip'] = contains_ip\n",
    "                setup_data['contains_domain'] = contains_domain\n",
    "                setup_data['contains_bytestrings'] = contains_bytestrings\n",
    "                setup_data['contains_base64'] = contains_base64\n",
    "                setup_data['contains_eval'] = contains_eval\n",
    "                setup_data['contains_import_subprocess'] = contains_import_subprocess\n",
    "                setup_data['contains_import_os'] = contains_import_os\n",
    "                setup_data['contains_import_network_modules'] = contains_import_network_modules\n",
    "                setup_data['contains_os_environ_access'] = contains_os_environ_access\n",
    "\n",
    "                with open(setup_json_file, 'w') as f:\n",
    "                    json.dump(setup_data, f, indent=4)\n",
    "                print(f\"Updated {setup_json_file} with extracted features\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating {setup_json_file}: {e}\")\n",
    "\n",
    "extract_features_from_xml(dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosntruct dependency graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Add code to construct dependency graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to dataframe with option to save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the packages\n",
    "# dataset_dir = '/mnt/volume_nyc1_01/benignPyPI'\n",
    "\n",
    "# Call the function and get the DataFrame\n",
    "df = read_json_files(dataset_dir)\n",
    "\n",
    "# columns_to_drop = ['package_size_bytes', 'package_size']\n",
    "# df = df.drop(columns=columns_to_drop)\n",
    "\n",
    "# #save to CSV\n",
    "df.to_csv('maliciousPyPI.csv', index=False)\n",
    "\n",
    "# #load df from CSV\n",
    "# df = pd.read_csv('benignPyPI.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
