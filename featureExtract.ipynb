{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import ast\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import logging\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "logstd = logging.StreamHandler(sys.stdout)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s:%(lineno)d - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S %Z\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[logstd]\n",
    ")\n",
    "\n",
    "log = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_setup_json_files(directory):\n",
    "    setup_json_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'setup.json':\n",
    "                setup_json_files.append(os.path.join(root, file))\n",
    "    return setup_json_files\n",
    "\n",
    "def is_valid_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json.load(file)\n",
    "        return True\n",
    "    except (ValueError, json.JSONDecodeError):\n",
    "        return False\n",
    "\n",
    "def count_valid_json_files(directory):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == \"setup.json\" and is_valid_json_file(os.path.join(root, file)):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def count_package_files(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".tar.gz\") or file.endswith(\".tar.bz2\") or file.endswith(\".tar.xz\"):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def is_valid_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json.load(file)\n",
    "        return True\n",
    "    except (ValueError, json.JSONDecodeError):\n",
    "        return False\n",
    "\n",
    "def count_valid_json_files(directory):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == \"setup.json\" and is_valid_json_file(os.path.join(root, file)):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def find_setup_json_files(directory):\n",
    "    setup_json_files = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'setup.json':\n",
    "                setup_json_files.append(os.path.join(root, file))\n",
    "    return setup_json_files\n",
    "\n",
    "def find_python_files(directory):\n",
    "    python_files = []\n",
    "    for entry in os.scandir(directory):\n",
    "        if entry.is_file() and entry.name.endswith('.py') and not entry.name.startswith('.'):\n",
    "            python_files.append(entry.path)\n",
    "        elif entry.is_dir():\n",
    "            python_files.extend(find_python_files(entry.path))\n",
    "    return python_files\n",
    "\n",
    "def read_json_files(directory):\n",
    "    # Initialize an empty list to hold the JSON data\n",
    "    json_data_list = []\n",
    "\n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'setup.json':\n",
    "                # Construct the full file path\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read the JSON file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    json_data_list.append(json_data)\n",
    "\n",
    "    # Convert the list of JSON data to a DataFrame\n",
    "    df = pd.DataFrame(json_data_list)\n",
    "    return df\n",
    "\n",
    "dataset_dir = \"/mnt/volume_nyc1_01/benignPyPI\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count zipped packages to determine useable amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_dir = \"/mnt/volume_nyc1_01/Backstabbers-Knife-Collection/samples\"\n",
    "dataset_2_dir = \"/mnt/volume_nyc1_01/pypi_malregistry\"  \n",
    "\n",
    "count_1 = count_package_files(dataset_1_dir)\n",
    "count_2 = count_package_files(dataset_2_dir)\n",
    "\n",
    "print(f\"Number of packages in dataset 1: {count_1}\")\n",
    "print(f\"Number of packages in dataset 2: {count_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count JSON files to determin useable amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1_dir = \"/mnt/volume_nyc1_01/Backstabbers-Knife-Collection/samples\"\n",
    "dataset_2_dir = \"/mnt/volume_nyc1_01/pypi_malregistry\"  \n",
    "\n",
    "count_1 = count_valid_json_files(dataset_1_dir)\n",
    "count_2 = count_valid_json_files(dataset_2_dir)\n",
    "\n",
    "print(f\"Number of valid setup.json files in dataset 1: {count_1}\")\n",
    "print(f\"Number of valid setup.json files in dataset 2: {count_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Shannon Entropy and append to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(directory):\n",
    "    package_entropies = {}\n",
    "    setup_json_files = find_setup_json_files(directory)\n",
    "    \n",
    "    for setup_file_path in setup_json_files:\n",
    "        package_path = os.path.dirname(setup_file_path)\n",
    "        package_name = os.path.basename(package_path)\n",
    "        \n",
    "        package_entropy = 0\n",
    "        total_files = 0\n",
    "        \n",
    "        python_files = find_python_files(package_path)\n",
    "        for file_path in python_files:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                    text = f.read()\n",
    "                    freqs = np.array(list(Counter(text).values()))\n",
    "                    probs = freqs / len(text)\n",
    "                    entropy_value = entropy(probs, base=2)\n",
    "                    package_entropy += entropy_value\n",
    "                    total_files += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        if total_files > 0:\n",
    "            average_entropy = package_entropy / total_files\n",
    "            package_entropies[package_name] = average_entropy\n",
    "\n",
    "            try:\n",
    "                with open(setup_file_path, 'r+', encoding='utf-8', errors='ignore') as setup_file:\n",
    "                    try:\n",
    "                        setup_data = json.load(setup_file)\n",
    "                        setup_data[\"average_entropy\"] = average_entropy\n",
    "                        setup_file.seek(0)\n",
    "                        json.dump(setup_data, setup_file, indent=4)\n",
    "                        setup_file.truncate()\n",
    "                        print(f\"Updated {setup_file_path} with average entropy: {average_entropy}\")\n",
    "                    except json.JSONDecodeError as json_err:\n",
    "                        print(f\"JSON decode error in {setup_file_path}: {json_err}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating {setup_file_path}: {e}\")\n",
    "\n",
    "    return package_entropies\n",
    "\n",
    "setup_json_files = find_setup_json_files(dataset_dir)\n",
    "\n",
    "if setup_json_files:\n",
    "    print(\"Found setup.json files:\")\n",
    "    for file in setup_json_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"No setup.json files found in the specified directory.\")\n",
    "\n",
    "package_entropies = shannon_entropy(dataset_dir)\n",
    "for package, entropy in package_entropies.items():\n",
    "    print(f\"Shannon entropy of {package}: {entropy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct AST and store in XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_ast(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        content = f.read()\n",
    "        try:\n",
    "            tree = ast.parse(content)\n",
    "            return tree\n",
    "        except SyntaxError as e:\n",
    "            print(f\"SyntaxError in {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def _convert(node, parent):\n",
    "    if isinstance(node, ast.AST):\n",
    "        node_name = node.__class__.__name__\n",
    "        element = ET.SubElement(parent, node_name)\n",
    "        for field, value in ast.iter_fields(node):\n",
    "            field_elem = ET.SubElement(element, field)\n",
    "            _convert(value, field_elem)\n",
    "    elif isinstance(node, list):\n",
    "        for item in node:\n",
    "            item_elem = ET.SubElement(parent, 'item')\n",
    "            _convert(item, item_elem)\n",
    "    else:\n",
    "        parent.text = str(node)\n",
    "\n",
    "def ast_to_xml(node):\n",
    "    root = ET.Element(node.__class__.__name__)\n",
    "    _convert(node, root)\n",
    "    return root\n",
    "\n",
    "def find_python_files(directory):\n",
    "    python_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                python_files.append(os.path.join(root, file))\n",
    "    return python_files\n",
    "\n",
    "def save_xml(xml, file_path):\n",
    "    xml_str = ET.tostring(xml, encoding='unicode', method='xml')\n",
    "    xml_file_path = os.path.splitext(file_path)[0] + '.xml'\n",
    "    with open(xml_file_path, 'w', encoding='utf-8', errors='replace') as f:\n",
    "        f.write(xml_str)\n",
    "\n",
    "python_files = find_python_files(dataset_dir)\n",
    "\n",
    "for file in python_files:\n",
    "    tree = construct_ast(file)\n",
    "    if tree is not None:\n",
    "        xml = ast_to_xml(tree)\n",
    "        save_xml(xml, file)\n",
    "        print(f\"XML representation saved for {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_python_files(directory):\n",
    "    python_files = [entry.path for entry in os.scandir(directory) if entry.is_file() and entry.name.endswith('.py')]\n",
    "    return len(python_files)\n",
    "\n",
    "def count_python_files_in_packages(dataset_directory):\n",
    "    package_counts = {}\n",
    "    \n",
    "    for entry in os.scandir(dataset_directory):\n",
    "        if entry.is_dir() and any(sub_entry.name == '__init__.py' for sub_entry in os.scandir(entry.path)):\n",
    "            package_name = entry.name\n",
    "            package_counts[package_name] = count_python_files(entry.path)\n",
    "    \n",
    "    return package_counts\n",
    "\n",
    "dataset_directory = '/mnt/volume_nyc1_01/benignPyPI/'\n",
    "\n",
    "package_python_file_counts = count_python_files_in_packages(dataset_directory)\n",
    "\n",
    "for package, count in package_python_file_counts.items():\n",
    "    print(f\"Package '{package}' has {count} Python files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to dataframe with option to save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the packages\n",
    "directory = '/mnt/volume_nyc1_01/benignPyPI'\n",
    "\n",
    "# Call the function and get the DataFrame\n",
    "df = read_json_files(directory)\n",
    "\n",
    "# #save to CSV\n",
    "# df.to_csv('benignPyPI.csv', index=False)\n",
    "\n",
    "# #load df from CSV\n",
    "# df = pd.read_csv('benignPyPI.csv')\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
